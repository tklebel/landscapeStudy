---
title: "Analysis Writeup"
author: "Thomas Klebel"
date: "7/2/2019"
output: 
  bookdown::html_document2:
    number_sections: false
    keep_md: true
bibliography: landscape.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, dpi = 400)
```

```{r import, message=FALSE}
library(tidyverse)
library(RColorBrewer)
library(here)
library(landscapeStudy)
# install.packages("ggchicklet", repos = "https://cinc.rud.is")
library(ggchicklet)
library(tidytext)
library(ggraph)


theme_set(hrbrmisc::theme_hrbrmstr())


# import data
refined <- read_csv(here("data-transformed/refined.csv"))
refined_with_areas <- read_csv(here("data-transformed/refined_w_areas.csv"))

# tweak factor ordering
refined_with_areas <- refined_with_areas %>% 
  order_factors()

```

# Subject areas
The approach taken to create the sample of journals led to a few journals 
having no data on subject area: some journals like "Gut" were within the top
100 journals, but not within any of the sub-categories. This is because the
h-index varies greatly between sub-categories, as can be seen from figure 
\@ref(fig:h-indices).

```{r h-indices, fig.cap="Distribution of h5-index across disciplines"}
# set seed for jittered points to stay always the same (otherwise annoying for
# git)
set.seed(1234)

refined_with_areas %>% 
  filter(!area_was_scraped) %>% 
  ggplot(aes(fct_reorder(area, `h5-index`), `h5-index`)) +
  geom_boxplot() +
  coord_flip() +
  geom_jitter(width = .2, aes(colour = area), show.legend = F, alpha = .7) +
  labs(x = NULL) +
  scale_color_brewer(palette = "Dark2") 
  
```

To be able to include all journals for analysis that splits by discipline, the
missing categorisations were added afterwards. To this end, we scraped all 
disciplines and subdisciplines from GoogleScholar and matched those to our data.
^[The code for collecting the data from GoogleScholar can be found here:
ADD LINKS HERE TO DATA AND SCRIPT]

As stated, the criteria for inclusion into the GoogleScholar rankings are opaque
and non-reproducible. For example it is possible for a journal to be included in
different disciplines, which makes a lot of sense
(for example "Physics & Mathematics" along with 
"Engineering & Computer Science"). It is however also possible for a journal to
be included in a sub-discipline, and not in the parent discipline, despite 
having a higher h-index than all journals listed in the parten discipline.^[As
of 2019-07-02, the "Journal of Cleaner Production" is listed in the social 
sciences under "sustainable development"
(https://scholar.google.at/citations?view_op=top_venues&hl=en&vq=soc_sustainabledevelopment). 
But it is not listed under the parent category 
(https://scholar.google.at/citations?view_op=top_venues&hl=en&vq=soc).]


```{r sample-characteristics}
n_double <- refined_with_areas %>% 
  count(title, sort = T) %>% 
  filter(n > 1) %>% 
  nrow()
```


The nature of our selection means that 
`r glue::glue("{n_double} out of {nrow(refined)}")` journals are assigned to two
disciplines. The inclusion criteria further mean, that disciplines are not 
represented equally in the sample. Since many of the top 100 journals belong to 
the health and medical sciences, the sample is slightly skewed in that direction
(see figure \@ref(fig:sample-skew)).


```{r sample-skew, fig.cap="Sampled journals by discipline", fig.height=4, fig.width=7}
refined_with_areas %>%
  plot_univariate(area, nudge_y = .5) +
  coord_flip() +
  labs(title = NULL) +
  hrbrmisc::theme_hrbrmstr(grid = "") +
  theme(axis.text.x = element_blank()) +
  aes(colour = area) +
  scale_color_brewer(palette = "Dark2")
```


# Peer Review
```{r pr-computation}
peer_type_data <- refined_with_areas %>% 
  make_proportion(pr_type_clean, area, order_string = "Double|Single")

# what is the percentage of "unsure"?
perc_unsure <- refined_with_areas %>% 
  make_proportion(pr_type_clean, area, order_string = "Unsure") %>% 
  ungroup() %>% 
  summarise(unsure = mean(order) %>%  scales::percent(., accuracy = 1))  %>% 
  as.character()

```

Information on what type of peer review is used by a journal is mixed 
(see figure \@ref(fig:peer-type)).
Overall, around `r perc_unsure` of all journals do not provide clear information
about their peer review process. However, there are major differences between 
disciplines. In the social sciences and humanities, double blind peer review is
more prevalent, and the proportion of unclear policies the lowest. Business,
Economics and Management, along with most disciplines from the sciences, display
higher levels of unclear policies, with single blind peer review being more 
prevalent in the sciences.

```{r peer-type, fig.cap="Type of peer review used", fig.asp=.6}
p_cols <- c("Single blind" = "#1B9E77", "Double blind" =  "#D95F02",
            "Not blinded" =  "#7570B3",
            "Unsure" = "#A6761D", "Other" = "#666666")

ggplot(peer_type_data, aes(fct_reorder(area, order), prop, fill = fct_rev(pr_type_clean))) +
  geom_chicklet(position = "fill", width = .6) +
  coord_flip() +
  scale_fill_manual(values = p_cols) +
  scale_y_continuous(labels = scales::percent) +
  theme(legend.position = "top") +
  guides(fill = guide_legend(reverse = T)) +
  labs(fill = NULL, x = NULL, y = NULL, 
       caption = "What type of peer review is used?")
```

```{r pr-stats}
pr_recognition <- refined %>% 
  make_single_proportion(pr_database, "Yes")

pr_unsure <- refined %>% 
  make_single_proportion(pr_database, "Unsure")
```


When it comes to recognition of peer review activity, only `r pr_recognition` of
all journals deposit reviewer activity into open datdabases. On the other hand,
the majority of journals (`r pr_unsure`) does not state at all, whether peer 
review activity is deposited in any kind of database, open or not. 

# Open Peer Review
```{r opr-computation}
pdata <- refined %>% 
  select(opr_reports:opr_interaction) %>% 
  gather(var, val) %>% 
  mutate(val_clean = case_when(
    str_detect(val, "Conditional") ~ "Conditional",
    str_detect(str_to_lower(val), "not spec") ~ "Not specified",
    str_detect(val, "Optional") ~ "Optional",
    TRUE ~ val
  )) %>% 
  make_proportion(val_clean, var, order_string = "Yes|Mand|Condi|Optio") %>% 
  mutate(val_clean = factor(val_clean, 
                      levels = c("Yes", "Mandatory", "Conditional", "Optional",
                                 "No", "Not specified")))

labels <- pdata %>% 
  distinct(var) %>% 
  mutate(label = case_when(
    var == "opr_reports" ~ "Are peer review reports being published?",
    var == "opr_responses" ~ "Are author responses to reviews being published?",
    var == "opr_letters" ~ "Are editorial decision letters being published?",
    var == "opr_versions" ~ "Are previous versions of the manuscript being published?",
    var == "opr_identities_published" ~ "Are reviewer identities being published?",
    var == "opr_indenties_author" ~ "Are reviewer identities revealed to the author (even if not published)?",
    var == "opr_comments" ~ "Is there public commenting during formal peer review?",
    var == "opr_interaction" ~ "Is there open interaction (reviewers consult with one another)?"
  )) %>% 
mutate_at("label", ~str_wrap(., 40))

```

Information on open peer review is similarly sparse (see fig. \@ref(fig:opr)). 
All surveyed aspects of 
open peer review lack any kind of information in more than 50% of journals 
surveyed.
Furthermore, seven out of eight aspects are not specified in more than three 
quarters of all journals. When there is information, in most cases it is 
dismissive of open peer review. No journal in our sample allows public 
commenting during formal peer review. Other forms of openness are similarly 
scarce, with the only expection that some journals may reveal reviewer 
identities to the authors.

```{r opr, fig.asp=.8, fig.cap="Aspects of open peer review"}
p_cols <- c("Not specified" =  "#E7298A", "Yes" = "#1B9E77",
            "Mandatory" = "#D95F02", "Conditional" =  "#7570B3",
            "Optional" = "#66A61E", "No" = "#E6AB02"
)

pdata %>% 
  left_join(labels) %>% 
  ggplot(., aes(fct_reorder(label, order), prop, fill = fct_rev(val_clean))) +
  geom_chicklet(position = "fill", width = .6) +
  coord_flip() +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_manual(values = p_cols) +
  theme(legend.position = "top") +
  guides(fill = guide_legend(reverse = T)) +
  labs(x = NULL, y = NULL, fill = NULL) 
```

Splitting the aspect of revealed reviewer identities by discipline shows a few
key distictions (see fig. \@ref(fig:opr-authors)). Whereas revealing reviewer
identities to the authors is absent from the social sciences, humanities and
business, it is not unusual in the sciences, at least on an optional basis 
(for example in case the referee requests it).


```{r opr-authors, fig.cap="Open reviewer identities towards authors"}
rev_identitiy <- refined_with_areas %>% 
  mutate(opr_indenties_author_clean = case_when(
    str_detect(opr_indenties_author, "Conditional") ~ "Conditional",
    str_detect(str_to_lower(opr_indenties_author), "not spec") ~ "Not specified",
    str_detect(opr_indenties_author, "Optional") ~ "Optional",
    TRUE ~ opr_indenties_author
  )) %>% 
  mutate(opr_indenties_author_clean = factor(opr_indenties_author_clean, 
                      levels = c("Yes", "Mandatory", "Conditional", "Optional",
                                 "No", "Not specified"))) %>% 
  make_proportion(opr_indenties_author_clean, area, order_string = "Not")


p_cols <- c("Not specified" =  "#E7298A", "Yes" = "#1B9E77",
            "Mandatory" = "#D95F02", "Conditional" =  "#7570B3",
            "Optional" = "#66A61E", "No" = "#E6AB02"
)

ggplot(rev_identitiy, aes(fct_rev(fct_reorder(area, order)), prop, 
                          fill = fct_rev(opr_indenties_author_clean))) +
  geom_chicklet(position = "fill", width = .6) +
  coord_flip() +
  scale_fill_manual(values = p_cols) +
  scale_y_continuous(labels = scales::percent) +
  theme(legend.position = "top") +
  guides(fill = guide_legend(reverse = T)) +
  labs(fill = NULL, x = NULL, y = NULL, 
       caption = "Are reviewer identities revealed to the author (even if not published)?")
```

# Co-Review Policy
```{r}
coreview_policies <- refined %>% 
  select(coreview_policy) %>% 
  filter(!is.na(coreview_policy),
         !(coreview_policy %in% c("Not specified", "Not found")))
```
Information on coreview policies is sparse. 
Only `r nrow(coreview_policies)` out of `r nrow(refined)` journals do have an
explicit coreview.

Splitting the results by discipline
reveals noticable differences (see fig. \@ref(fig:co-rev)).
While in the life and earth sciences more
than one third of journals permit contributions from co-reviewers, in the 
humanities, chemical & materials sciences, and in business, economics & 
management about 90% of journals have no policy on co-reviewing.

```{r co-rev, fig.cap="Prevalence of co-review"}
co_rev <- refined_with_areas %>% 
  mutate(coreview_email = case_when(
    coreview_email == "unsure" ~ "Unsure",
    # lump the two not specified to unsure, since this is similar in this 
    # instance
    coreview_email == "Not specified" ~ "Unsure",
    TRUE ~ coreview_email
  ),
  coreview_email = factor(coreview_email, 
                      levels = c("Yes", "No", "Unsure"))) %>% 
  make_proportion(coreview_email, area, order_string = "Yes")


p_cols <- c("Unsure" = "#A6761D", "Yes" = "#1B9E77",
            "No" = "#E6AB02"
)

ggplot(co_rev, aes(fct_reorder(area, order), prop, 
                          fill = fct_rev(coreview_email))) +
  geom_chicklet(position = "fill", width = .6) +
  coord_flip() +
  scale_fill_manual(values = p_cols) +
  scale_y_continuous(labels = scales::percent) +
  theme(legend.position = "top") +
  guides(fill = guide_legend(reverse = T)) +
  labs(fill = NULL, x = NULL, y = NULL, 
       caption = "Can co-reviewers contribute?")
```


To obtain a more nuanced view of the policies' contents, we also analysed their
full text via text mining. Due to policies being similar across journals of 
certain publishers, there are `r nrow(distinct(coreview_policies))`
distinct policies in our dataset (compared to `r nrow(coreview_policies)` 
policies in total). Since the policies are rather short, we are 
somewhat limited in regard to what insight we can gain from automated 
procedures.


```{r}
custom_stopwords <- tidytext::stop_words %>% 
  filter(word != "not")

stopped_words <- coreview_policies %>% 
  distinct() %>%
  unnest_tokens(word, coreview_policy) %>% 
  anti_join(custom_stopwords)
```

To extract meaningful information we first removed common words of the english
language (via the list of stopwords from the tidytext package
[@silge_tidytext_2016], except for the word "not", which is relevant since some
policies state, that it is *not* appropriate to share information with students
or colleagues). The resulting list contains `r nrow(stopped_words)` words in 
total. 

For a simple overview, the words were stemmed to reduce similar but not 
identical versions of certain words (like editor/editors).
The following table displays the most frequent parts of the distinct policies,
sorted by propensity.
```{r}
stopped_words %>% 
  mutate(word = SnowballC::wordStem(word)) %>%
  count(word, sort = T) %>% 
  head(20) %>% 
  knitr::kable()
```

The most prominent themes that emerge are:

- Individuals with varying stakes regarding peer review: editor, colleague, 
author, student, peer
- Confidentiality as a central principle
- Important elements of scholarly publishing: manuscript, journal, review, 
process
- Verbal forms pertaining to relationships between the individuals: inform,
involve, consult, discuss, disclose, ensure, obtain

These directions become more intelligible when we look at bigrams (see fig.
\@ref(fig:bigrams)). With this procedure the text is
split into pairs of words (for example the sentence "All humans are equal" 
becomes "All humans", "humans are", "are equal"). The most prominent bigrams 
where "peer -> review" and "review -> process". To take a look at the strength 
of other associations, the term "review" was removed from the figure.


```{r bigrams, fig.width=9, fig.height=7, fig.cap="Bigrams of co-review policies"}
coreview_policies %>% 
  make_bigram_analysis(coreview_policy, remove = "review", cutoff = 2)
```

From both displays it is obvious, that journals stress the importance of 
"maintaining confidentiality", by "not shar[ing]" or disclosing information,
neither to "junior researchers", "laboratory colleagues" nor "graduate 
students". Even if the policies do not explicitly forbid or allow the 
involvement of other researchers, in many cases they mandate the reviewer to 
first obtain permission from the editor in case they want to involve someone
else in their review.

# Preprints
```{r}
clean_preprint_version <- function(df) {
  df %>% 
    mutate(preprint_version_clean = case_when(
      str_detect(preprint_version, "Unsure") ~ 
        "Unsure (preprints are allowed, but it's not clear which version)",
      str_detect(preprint_version, "Any .*? or") ~ "Other",
      str_detect(preprint_version, "Other|Unclear") ~ "Other",
      str_detect(preprint_version, "None") ~ "None",
      str_detect(preprint_version, "First sub") ~ 
        "First submission only (before peer review)",
      str_detect(preprint_version, "After peer re") ~ "After peer review",
      str_detect(preprint_version, "Any|any") ~ "Any",
      str_detect(preprint_version, "No") ~ "No preprint policy",
    )) 
}

preprint_version_univariate <- refined %>% 
  clean_preprint_version() %>% 
  make_proportion(preprint_version_clean, order_string = "Any|After|First|Uns")
```


Preprints are more common within our sample than open peer review or co-review
policies. Almost
`r preprint_version_univariate %>% pull(order) %>% make_percent()`
of all journals allow preprints at least in some way. Most of them however only
allow preprints before peer review 
(`r preprint_version_univariate[[3, 3]] %>% make_percent(2)`)
while 
`r preprint_version_univariate[[4, 3]] %>% make_percent(2)`
do not have a preprint policy.


```{r preprint-version, fig.cap="Allowed preprint versions", fig.asp=.65}
refined_with_areas <- refined_with_areas %>% 
  clean_preprint_version() %>% 
  mutate(preprint_version_clean = fct_relevel(
    preprint_version_clean, "Any", "After peer review") %>% 
    fct_relevel("None", "No preprint policy", "Other", after = 4))
    
    
    
preprint_version <- refined_with_areas %>% 
  make_proportion(preprint_version_clean, area,
                  order_string = "Any|After|First|Uns")


p_cols <- c("No preprint policy" =  "#E7298A", "Other" = "#666666",
            "Any" = "#1B9E77", "After peer review" = "#D95F02", 
             "First submission only (before peer review)" = "#7570B3",
            "Unsure (preprints are allowed, but it's not clear which version)" = "#66A61E",
            "None" = "#E6AB02"
)

ggplot(preprint_version, aes(fct_reorder(area, order), prop, 
                          fill = fct_rev(preprint_version_clean))) +
  geom_chicklet(position = "fill", width = .6) +
  coord_flip() +
  scale_fill_manual(values = p_cols) +
  scale_y_continuous(labels = scales::percent) +
  theme(legend.position = "top") +
  guides(fill = guide_legend(reverse = T, nrow = 4)) +
  labs(fill = NULL, x = NULL, y = NULL, 
       caption = "What version of a preprint can be posted?")
```

Similar to our earlier results, preprint policies show a wide disciplinary 
range (see fig. \@ref(fig:preprint-version). While in the life sciences & earth 
sciences 
`r preprint_version %>% filter(str_detect(area, "Life Sciences")) %>% pull(order) %>% make_percent()`
of all journals allow preprints in some way, in the Humanities only 
`r preprint_version %>% filter(str_detect(area, "Humanities")) %>% pull(order) %>% make_percent(2)`
do.
The Sciences in general tend towards allowing preprints only on first submission
while the social sciences commonly have no clear policy on which version of a
preprint is allowed. 
The humanities and also journal from business, economics and management 
generally have either no preprint policy at all or are more diverse in regard to
preprint version, often allowing preprints after peer review, which is a lot 
less common in the sciences.

```{r}
clean_preprint_citation <- function(df) {
  df %>% 
   mutate(preprint_citation_clean = case_when(
      str_detect(preprint_citation, "Not spec") ~ "Not specified",
      str_detect(preprint_citation, "Unsure") ~ "Unsure",
      str_detect(preprint_citation, "No") ~ "No",
      str_detect(preprint_citation, "only in the text") ~ "Yes, but only in the text",
      str_detect(preprint_citation, "reference list") ~ "Yes, in the reference list",
      is.na(preprint_citation) ~ NA_character_,
      TRUE ~ "Other"
    )) 
}

preprint_citation_univariate <- refined %>% 
  clean_preprint_citation() %>% 
  make_proportion(preprint_citation_clean, order_string = "Yes")
```


A complementary aspect of using preprints is, whether they can be cited. The
majority of journals
(`r preprint_citation_univariate[[2, 3]] %>% make_percent(2)`)
does not specify, whether this is possible. Unclear policies are also quite 
common (`r preprint_citation_univariate[[4, 3]] %>% make_percent(2)`). In case
citations of preprints are allowed, this is commonly possible in the reference,
with some journals restricting citations of preprints to the text.


```{r preprint-citation, fig.cap="Citation of preprints", fig.asp=.65}
refined_with_areas <- refined_with_areas %>% 
  clean_preprint_citation()

preprint_citation <- refined_with_areas %>% 
    mutate(preprint_citation_clean = 
             fct_relevel(preprint_citation_clean,
                         "Yes, in the reference list", "Yes, but only in the text",
                         "No", "Unsure", "Other", "Not specified")) %>% 
  make_proportion(preprint_citation_clean, area,
                  order_string = "Yes")


p_cols <- c("Not specified" =  "#E7298A", "Other" = "#666666", "Unsure" = "#A6761D",
            "Yes, in the reference list" = "#1B9E77", 
            "Yes, but only in the text" = "#D95F02", 
            "No" = "#E6AB02"
)

ggplot(preprint_citation, aes(fct_reorder(area, order), prop, 
                          fill = fct_rev(preprint_citation_clean))) +
  geom_chicklet(position = "fill", width = .6) +
  coord_flip() +
  scale_fill_manual(values = p_cols) +
  scale_y_continuous(labels = scales::percent) +
  theme(legend.position = "top") +
  guides(fill = guide_legend(reverse = T)) +
  labs(fill = NULL, x = NULL, y = NULL, 
       caption = "Can preprints be cited?")
```

Disciplinary differences are again very apparent (see fig.
\@ref(fig:preprint-citation)). Citing preprints is more common in the sciences,
with 
`r preprint_citation %>% filter(str_detect(area, "Life")) %>% pull(order) %>% make_percent()`
of all journals in the life and earth sciences allowing citations to preprints
either in the text or in the reference list. In contrast, the social sciences 
and humanities either have unclear or no policies regarding whether preprints 
can be cited or not. 


# Bibliography

