---
title: "Analysis Writeup"
author: "Thomas Klebel"
date: "7/2/2019"
output: 
  bookdown::html_document2:
    number_sections: false
    keep_md: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, dpi = 400)
```

```{r import, message=FALSE}
library(tidyverse)
library(RColorBrewer)
library(here)
library(landscapeStudy)
# install.packages("ggchicklet", repos = "https://cinc.rud.is")
library(ggchicklet)

theme_set(hrbrmisc::theme_hrbrmstr())


# import data
refined <- read_csv(here("data-transformed/refined.csv"))
refined_with_areas <- read_csv(here("data-transformed/refined_w_areas.csv"))

# tweak factor ordering
refined_with_areas <- refined_with_areas %>% 
  order_factors()

```

# Subject areas
The approach taken to create the sample of journals led to a few journals 
having no data on subject area: some journals like "Gut" were within the top
100 journals, but not within any of the sub-categories. This is because the
h-index varies greatly between sub-categories, as can be seen from figure 
\@ref(fig:h-indices).

```{r h-indices, fig.cap="Distribution of h5-index across disciplines"}
# set seed for jittered points to stay always the same (otherwise annoying for
# git)
set.seed(1234)

refined_with_areas %>% 
  filter(!area_was_scraped) %>% 
  ggplot(aes(fct_reorder(area, `h5-index`), `h5-index`)) +
  geom_boxplot() +
  coord_flip() +
  geom_jitter(width = .2, aes(colour = area), show.legend = F, alpha = .7) +
  labs(x = NULL) +
  scale_color_brewer(palette = "Dark2") 
  
```

To be able to include all journals for analysis that splits by discipline, the
missing categorisations were added afterwards. To this end, we scraped all 
disciplines and subdisciplines from GoogleScholar and matched those to our data.
^[The code for collecting the data from GoogleScholar can be found here:
ADD LINKS HERE TO DATA AND SCRIPT]

As stated, the criteria for inclusion into the GoogleScholar rankings are opaque
and non-reproducible. For example it is possible for a journal to be included in
different disciplines, which makes a lot of sense
(for example "Physics & Mathematics" along with 
"Engineering & Computer Science"). It is however also possible for a journal to
be included in a sub-discipline, and not in the parent discipline, despite 
having a higher h-index than all journals listed in the parten discipline.^[As
of 2019-07-02, the "Journal of Cleaner Production" is listed in the social 
sciences under "sustainable development"
(https://scholar.google.at/citations?view_op=top_venues&hl=en&vq=soc_sustainabledevelopment). 
But it is not listed under the parent category 
(https://scholar.google.at/citations?view_op=top_venues&hl=en&vq=soc).]


```{r sample-characteristics}
n_double <- refined_with_areas %>% 
  count(title, sort = T) %>% 
  filter(n > 1) %>% 
  nrow()
```


The nature of our selection means that 
`r glue::glue("{n_double} out of {nrow(refined)}")` journals are assigned to two
disciplines. The inclusion criteria further mean, that disciplines are not 
represented equally in the sample. Since many of the top 100 journals belong to 
the health and medical sciences, the sample is slightly skewed in that direction
(see figure \@ref(fig:sample-skew)).


```{r sample-skew, fig.cap="Sampled journals by discipline", fig.height=4, fig.width=7}
refined_with_areas %>%
  plot_univariate(area, nudge_y = .5) +
  coord_flip() +
  labs(title = NULL) +
  hrbrmisc::theme_hrbrmstr(grid = "") +
  theme(axis.text.x = element_blank()) +
  aes(colour = area) +
  scale_color_brewer(palette = "Dark2")
```


# Peer Review
```{r pr-computation}
peer_type_data <- refined_with_areas %>% 
  make_proportion(pr_type_clean, area, order_string = "Double|Single")

# what is the percentage of "unsure"?
perc_unsure <- refined_with_areas %>% 
  make_proportion(pr_type_clean, area, order_string = "Unsure") %>% 
  ungroup() %>% 
  summarise(unsure = mean(order) %>%  scales::percent(., accuracy = 1))  %>% 
  as.character()

```

Information on what type of peer review is used by a journal is mixed 
(see figure \@ref(fig:peer-type)).
Overall, around `r perc_unsure` of all journals do not provide clear information
about their peer review process. However, there are major differences between 
disciplines. In the social sciences and humanities, double blind peer review is
more prevalent, and the proportion of unclear policies the lowest. Business,
Economics and Management, along with most disciplines from the sciences, display
higher levels of unclear policies, with single blind peer review being more 
prevalent in the sciences.

```{r peer-type, fig.cap="Type of peer review used", fig.asp=.6}
p_cols <- c("Single blind" = "#1B9E77", "Double blind" =  "#D95F02",
            "Not blinded" =  "#7570B3",
            "Unsure" = "#A6761D", "Other" = "#666666")

ggplot(peer_type_data, aes(fct_reorder(area, order), prop, fill = pr_type_clean)) +
  geom_chicklet(position = "fill", width = .6) +
  coord_flip() +
  scale_fill_manual(values = p_cols) +
  scale_y_continuous(labels = scales::percent) +
  theme(legend.position = "top") +
  labs(fill = NULL, x = NULL, y = NULL, 
       caption = "What type of peer review is used?")
```

```{r pr-stats}
pr_recognition <- refined %>% 
  make_single_proportion(pr_database, "Yes")

pr_unsure <- refined %>% 
  make_single_proportion(pr_database, "Unsure")
```


When it comes to recognition of peer review activity, only `r pr_recognition` of
all journals deposit reviewer activity into open datdabases. On the other hand,
the majority of journals (`r pr_unsure`) does not state at all, whether peer 
review activity is deposited in any kind of database, open or not. 

# Open Peer Review
```{r opr-computation}
pdata <- refined %>% 
  select(opr_reports:opr_interaction) %>% 
  gather(var, val) %>% 
  mutate(val_clean = case_when(
    str_detect(val, "Conditional") ~ "Conditional",
    str_detect(str_to_lower(val), "not spec") ~ "Not specified",
    str_detect(val, "Optional") ~ "Optional",
    TRUE ~ val
  )) %>% 
  make_proportion(val_clean, var, order_string = "Yes|Mand|Condi|Optio") %>% 
  mutate(val_clean = factor(val_clean, 
                      levels = c("Yes", "Mandatory", "Conditional", "Optional",
                                 "No", "Not specified")))

labels <- pdata %>% 
  distinct(var) %>% 
  mutate(label = case_when(
    var == "opr_reports" ~ "Are peer review reports being published?",
    var == "opr_responses" ~ "Are author responses to reviews being published?",
    var == "opr_letters" ~ "Are editorial decision letters being published?",
    var == "opr_versions" ~ "Are previous versions of the manuscript being published?",
    var == "opr_identities_published" ~ "Are reviewer identities being published?",
    var == "opr_indenties_author" ~ "Are reviewer identities revealed to the author (even if not published)?",
    var == "opr_comments" ~ "Is there public commenting during formal peer review?",
    var == "opr_interaction" ~ "Is there open interaction (reviewers consult with one another)?"
  )) %>% 
mutate_at("label", ~str_wrap(., 40))

```

Information on open peer review is similarly sparse (see fig. \@ref(fig:opr)). 
All surveyed aspects of 
open peer review lack any kind of information in more than 50% of journals 
surveyed.
Furthermore, seven out of eight aspects are not specified in more than three 
quarters of all journals. When there is information, in most cases it is 
dismissive of open peer review. No journal in our sample allows public 
commenting during formal peer review. Other forms of openness are similarly 
scarce, with the only expection that some journals may reveal reviewer 
identities to the authors.

```{r opr, fig.asp=.8, fig.cap="Aspects of open peer review"}
p_cols <- c("Not specified" =  "#E7298A", "Yes" = "#1B9E77",
            "Mandatory" = "#D95F02", "Conditional" =  "#7570B3",
            "Optional" = "#66A61E", "No" = "#E6AB02"
)

pdata %>% 
  left_join(labels) %>% 
  ggplot(., aes(fct_reorder(label, order), prop, fill = fct_rev(val_clean))) +
  geom_chicklet(position = "fill", width = .6) +
  coord_flip() +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_manual(values = p_cols) +
  theme(legend.position = "top") +
  guides(fill = guide_legend(reverse = T)) +
  labs(x = NULL, y = NULL, fill = NULL) 
```

Splitting the aspect of revealed reviewer identities by discipline shows a few
key distictions (see fig. \@ref(fig:opr-authors)). Whereas revealing reviewer
identities to the authors is absent from the social sciences, humanities and
business, it is not unusual in the sciences, at least on an optional basis 
(for example in case the referee requests it).


```{r opr-authors, fig.cap="Open reviewer identities towards authors"}
rev_identitiy <- refined_with_areas %>% 
  mutate(opr_indenties_author_clean = case_when(
    str_detect(opr_indenties_author, "Conditional") ~ "Conditional",
    str_detect(str_to_lower(opr_indenties_author), "not spec") ~ "Not specified",
    str_detect(opr_indenties_author, "Optional") ~ "Optional",
    TRUE ~ opr_indenties_author
  )) %>% 
  mutate(opr_indenties_author_clean = factor(opr_indenties_author_clean, 
                      levels = c("Yes", "Mandatory", "Conditional", "Optional",
                                 "No", "Not specified"))) %>% 
  make_proportion(opr_indenties_author_clean, area, order_string = "Not")


p_cols <- c("Not specified" =  "#E7298A", "Yes" = "#1B9E77",
            "Mandatory" = "#D95F02", "Conditional" =  "#7570B3",
            "Optional" = "#66A61E", "No" = "#E6AB02"
)

ggplot(rev_identitiy, aes(fct_rev(fct_reorder(area, order)), prop, 
                          fill = fct_rev(opr_indenties_author_clean))) +
  geom_chicklet(position = "fill", width = .6) +
  coord_flip() +
  scale_fill_manual(values = p_cols) +
  scale_y_continuous(labels = scales::percent) +
  theme(legend.position = "top") +
  guides(fill = guide_legend(reverse = T)) +
  labs(fill = NULL, x = NULL, y = NULL, 
       caption = "Are reviewer identities revealed to the author (even if not published)?")
```




