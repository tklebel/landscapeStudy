---
title: "Analysis Writeup"
author: "Thomas Klebel"
date: "7/2/2019"
output: 
  bookdown::html_document2:
    number_sections: false
    keep_md: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, dpi = 400)
```

```{r, message=FALSE}
library(tidyverse)
library(RColorBrewer)
library(here)
library(landscapeStudy)
# install.packages("ggchicklet", repos = "https://cinc.rud.is")
library(ggchicklet)

theme_set(hrbrmisc::theme_hrbrmstr())


# import data
refined <- read_csv(here("data-transformed/refined.csv"))
refined_with_areas <- read_csv(here("data-transformed/refined_w_areas.csv"))

# tweak factor ordering
refined_with_areas <- refined_with_areas %>% 
  order_factors()

```

# Subject areas
The approach taken to create the sample of journals led to a few journals 
having no data on subject area: some journals like "Gut" were within the top
100 journals, but not within any of the sub-categories. This is because the
h-index varies greatly between sub-categories, as can be seen from figure 
\@ref(fig:h-indices).

```{r h-indices, fig.cap="Distribution of h5-index across disciplines"}
# set seed for jittered points to stay always the same (otherwise annoying for
# git)
set.seed(1234)

refined_with_areas %>% 
  filter(!area_was_scraped) %>% 
  ggplot(aes(fct_reorder(area, `h5-index`), `h5-index`)) +
  geom_boxplot() +
  coord_flip() +
  geom_jitter(width = .2, aes(colour = area), show.legend = F, alpha = .7) +
  labs(x = NULL) +
  scale_color_brewer(palette = "Dark2") 
  
```

To be able to include all journals for analysis that splits by discipline, the
missing categorisations were added afterwards. To this end, we scraped all 
disciplines and subdisciplines from GoogleScholar and matched those to our data.
^[The code for collecting the data from GoogleScholar can be found here:
ADD LINKS HERE TO DATA AND SCRIPT]

As stated, the criteria for inclusion into the GoogleScholar rankings are opaque
and non-reproducible. For example it is possible for a journal to be included in
different disciplines, which makes a lot of sense
(for example "Physics & Mathematics" along with 
"Engineering & Computer Science"). It is however also possible for a journal to
be included in a sub-discipline, and not in the parent discipline, despite 
having a higher h-index than all journals listed in the parten discipline.^[As
of 2019-07-02, the "Journal of Cleaner Production" is listed in the social 
sciences under "sustainable development"
(https://scholar.google.at/citations?view_op=top_venues&hl=en&vq=soc_sustainabledevelopment). 
But it is not listed under the parent category 
(https://scholar.google.at/citations?view_op=top_venues&hl=en&vq=soc).]


```{r sample-characteristics}
n_double <- refined_with_areas %>% 
  count(title, sort = T) %>% 
  filter(n > 1) %>% 
  nrow()
```


The nature of our selection means that 
`r glue::glue("{n_double} out of {nrow(refined)}")` journals are assigned to two
disciplines. The inclusion criteria further mean, that disciplines are not 
represented equally in the sample. Since many of the top 100 journals belong to 
the health and medical sciences, the sample is slightly skewed in that direction
(see figure \@ref(fig:sample-skew)).


```{r sample-skew, fig.cap="Sampled journals by discipline", fig.height=4, fig.width=7}
refined_with_areas %>%
  plot_univariate(area, nudge_y = .5) +
  coord_flip() +
  labs(title = NULL) +
  hrbrmisc::theme_hrbrmstr(grid = "") +
  theme(axis.text.x = element_blank()) +
  aes(colour = area) +
  scale_color_brewer(palette = "Dark2")
```


# Peer Review
```{r}
peer_type_data <- refined_with_areas %>% 
  make_proportion(pr_type_clean, area, order_string = "double|single")

# what is the percentage of "unsure"?
perc_unsure <- refined_with_areas %>% 
  make_proportion(pr_type_clean, area, order_string = "unsure") %>% 
  ungroup() %>% 
  summarise(unsure = mean(order) %>%  scales::percent(., accuracy = 1))  %>% 
  as.character()

```

Information on what type of peer review is used by a journal is mixed 
(see figure \@ref(fig:peer-type)).
Overall, around `r perc_unsure` of all journals do not provide clear information
about their peer review process. However, there are major differences between 
disciplines. In the social sciences and humanities, double blind peer review is
more prevalent, and the proportion of unclear policies the lowest. Business,
Economics and Management, along with most disciplines from the sciences, display
higher levels of unclear policies, with single blind peer review being more 
prevalent in the sciences.

```{r peer-type, fig.cap="Type of peer review used", fig.asp=.6}
p_cols <- c("single blind" = "#1B9E77", "double blind" =  "#D95F02",
            "not blinded" =  "#7570B3",
            "unsure" = "#A6761D", "other" = "#666666")

ggplot(peer_type_data, aes(fct_reorder(area, order), prop, fill = pr_type_clean)) +
  geom_chicklet(position = "fill", width = .6) +
  coord_flip() +
  scale_fill_manual(values = p_cols) +
  scale_y_continuous(labels = scales::percent) +
  theme(legend.position = "top") +
  labs(fill = NULL, x = NULL, y = NULL)
```

```{r}
pr_recognition <- refined %>% 
  make_single_proportion(pr_database, "Yes")

pr_unsure <- refined %>% 
  make_single_proportion(pr_database, "Unsure")
```


When it comes to recognition of peer review activity, only `r pr_recognition` of
all journals deposit reviewer activity into open datdabases. On the other hand,
the majority of journals (`r pr_unsure`) does not state at all, whether peer 
review activity is deposited in any kind of database, open or not. 




