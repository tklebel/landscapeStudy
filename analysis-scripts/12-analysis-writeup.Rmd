---
title: "Analysis Write-up"
author: "Thomas Klebel"
date: Last updated `r lubridate::today()`
output: 
  bookdown::html_document2:
    number_sections: false
    keep_md: true
    toc: true
bibliography: landscape.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, dpi = 400)
```

```{r import, message=FALSE}
library(tidyverse)
library(RColorBrewer)
library(here)
library(landscapeStudy)
# install.packages("ggchicklet", repos = "https://cinc.rud.is")
library(ggchicklet)
library(tidytext)
library(ggraph)


theme_set(hrbrmisc::theme_hrbrmstr())


# import data
refined <- read_csv(here("data-transformed/refined.csv"))
refined_with_areas <- read_csv(here("data-transformed/refined_w_areas.csv"))

# tweak factor ordering
refined_with_areas <- refined_with_areas %>% 
  order_factors()

```

# Sample characteristics
The approach taken to create the sample of journals led to a few journals 
having no data on disciplinary area: some journals like "Gut" were within the 
top 100 journals, but not within any of the sub-categories. This is because the
h-index varies greatly between sub-categories. Figure \@ref(fig:h-indices)
shows the top-20 journals of each discipline.

```{r h-indices, fig.cap="Distribution of h5-index across disciplines", fig.asp=.6}
# set seed for jittered points to stay always the same (otherwise annoying for
# git)
set.seed(1234)

refined_with_areas %>% 
  filter(!area_was_scraped) %>% 
  ggplot(aes(fct_reorder(area, `h5-index`), `h5-index`)) +
  geom_boxplot(width = .6, outlier.alpha = 0) +
  coord_flip() +
  geom_jitter(width = .2, aes(colour = area), show.legend = F, alpha = .7) +
  labs(x = NULL) +
  scale_color_brewer(palette = "Dark2") 
  
```

The missing categorisations were added in a second step, to facilitate analysis 
of all journals that distinguishes by discipline. To this end, we scraped all 
disciplines and sub-disciplines from Google Scholar and matched those to our data.
^[The code for collecting the data from Google Scholar can be found here:
ADD LINKS HERE TO DATA AND SCRIPT]

As stated, the criteria for inclusion into the Google Scholar rankings are opaque
and non-reproducible. For example it is possible for a journal to be included in
different disciplines, which makes a lot of sense
(for example "Physics & Mathematics" along with 
"Engineering & Computer Science"). It is however also possible for a journal to
be included in a sub-discipline, and not in the parent discipline, despite 
having a higher h-index than all journals listed in the parent discipline.^[As
of 2019-07-02, the "Journal of Cleaner Production" is listed in the social 
sciences under "sustainable development"
(https://scholar.google.at/citations?view_op=top_venues&hl=en&vq=soc_sustainabledevelopment). 
But it is not listed under the parent category 
(https://scholar.google.at/citations?view_op=top_venues&hl=en&vq=soc).]


```{r sample-characteristics}
n_double <- refined_with_areas %>% 
  count(title, sort = T) %>% 
  filter(n > 1) %>% 
  nrow()
```


The nature of our selection means that 
`r glue::glue("{n_double} out of {nrow(refined)}")` journals are assigned to two
disciplines. The inclusion criteria further mean, that disciplines are not 
represented equally in the sample. Since many of the top 100 journals belong to 
the health and medical sciences, the sample is slightly skewed in that direction
(see figure \@ref(fig:sample-skew)).


```{r sample-skew, fig.cap="Sampled journals by discipline", fig.height=4, fig.width=7}
refined_with_areas %>%
  plot_univariate(area, nudge_y = .5) +
  coord_flip() +
  labs(title = NULL) +
  hrbrmisc::theme_hrbrmstr(grid = "") +
  theme(axis.text.x = element_blank()) +
  aes(colour = area) +
  scale_color_brewer(palette = "Dark2")
```




```{r}
oa_status <- refined %>% 
  summarise(has_oa = sum(!is.na(bibjson.oa_start.year)),
            n_total = n())
```


Regarding practices of open access, only `r oa_status$has_oa` of 
`r oa_status$n_total` journals are listed in the Directory of Open Access 
Journals (DOAJ) and can thus be considered fully open access. ^[Code and data 
for querying the DOAJ API and matching to our data can be found here FIXME]



# Peer Review
```{r pr-computation}
peer_type_data <- refined_with_areas %>% 
  make_proportion(pr_type_clean, area, order_string = "Double|Single")

# what is the percentage of "unsure"?
pr_type_prop <- refined %>% 
  make_proportion(pr_type_clean)

perc_unsure <- pr_type_prop %>% 
  filter(pr_type_clean == "Unsure") %>% 
  pull(prop) %>% 
  make_percent()

perc_not_blind <- pr_type_prop %>% 
  filter(pr_type_clean == "Not blinded") %>% 
  pull(prop) %>% 
  make_percent()

```

Information on what type of peer review is used by a journal is mixed 
(see figure \@ref(fig:peer-type-overall)).
Overall, more than `r perc_unsure` of all journals do not provide clear 
information about their peer review process. The most common peer review 
practice is single blind per review, followed by double blind peer review.
Some journals offer the option for authors to 
choose whether to use single or double blind peer review. These cases have been
coded as "Other" and amount to the majority of this category.  
`r perc_not_blind` journals do not anonymize papers or reviews during
review process.


```{r pr-type-overall}
p_cols <- c("Single blind" = "#1B9E77", "Double blind" =  "#D95F02",
            "Not blinded" =  "#7570B3",
            "Unsure" = "#A6761D", "Other" = "#666666")


ggplot(pr_type_prop, aes(fct_reorder(pr_type_clean, n), prop, fill = fct_rev(pr_type_clean))) +
  geom_chicklet(width = .6, show.legend = F) +
  coord_flip() +
  scale_fill_manual(values = p_cols) +
  scale_y_continuous(labels = function(x) scales::percent(x, accuracy = 1)) +
  theme(legend.position = "top") +
  guides(fill = guide_legend(reverse = T)) +
  labs(fill = NULL, x = NULL, y = NULL, 
       caption = "What type of peer review is used?")
```


However, there are major differences between disciplines (see figure 
\@ref(fig:peer-type)). In the social sciences, humanities, double blind peer
review is
more prevalent, and the proportion of unclear policies the lowest. Business,
Economics and Management, along with most disciplines from the sciences, display
higher levels of unclear policies, with single blind peer review being more 
prevalent in the sciences. 

```{r peer-type, fig.cap="Type of peer review used", fig.asp=.6}


ggplot(peer_type_data, aes(fct_reorder(area, order), prop, fill = fct_rev(pr_type_clean))) +
  geom_chicklet(position = "fill", width = .6) +
  coord_flip() +
  scale_fill_manual(values = p_cols) +
  scale_y_continuous(labels = scales::percent) +
  theme(legend.position = "top") +
  guides(fill = guide_legend(reverse = T)) +
  labs(fill = NULL, x = NULL, y = NULL, 
       caption = "What type of peer review is used?")
```

```{r pr-stats}
pr_recognition <- refined %>% 
  make_single_proportion(pr_database, "Yes")

pr_unsure <- refined %>% 
  make_single_proportion(pr_database, "Unsure")
```


When it comes to recognition of peer review activity, only `r pr_recognition` of
all journals deposit reviewer activity into open databases. Furthermore,
the majority of journals (`r pr_unsure`) does not state at all whether peer 
review activity is deposited in any kind of database, open or not. 

# Open Peer Review
```{r opr-computation}
pdata <- refined %>% 
  select(opr_reports:opr_interaction) %>% 
  gather(var, val) %>% 
  mutate(val_clean = case_when(
    str_detect(val, "Conditional") ~ "Conditional",
    str_detect(str_to_lower(val), "not spec") ~ "Not specified",
    str_detect(val, "Optional") ~ "Optional",
    TRUE ~ val
  )) %>% 
  make_proportion(val_clean, var, order_string = "Yes|Mand|Condi|Optio") %>% 
  mutate(val_clean = factor(val_clean, 
                      levels = c("Yes", "Mandatory", "Conditional", "Optional",
                                 "No", "Not specified")))

labels <- pdata %>% 
  distinct(var) %>% 
  mutate(label = case_when(
    var == "opr_reports" ~ "Are peer review reports being published?",
    var == "opr_responses" ~ "Are author responses to reviews being published?",
    var == "opr_letters" ~ "Are editorial decision letters being published?",
    var == "opr_versions" ~ "Are previous versions of the manuscript being published?",
    var == "opr_identities_published" ~ "Are reviewer identities being published?",
    var == "opr_indenties_author" ~ "Are reviewer identities revealed to the author (even if not published)?",
    var == "opr_comments" ~ "Is there public commenting during formal peer review?",
    var == "opr_interaction" ~ "Is there open interaction (reviewers consult with one another)?"
  )) %>% 
mutate_at("label", ~str_wrap(., 40))

```

Information on open peer review is similarly scarce (see fig. \@ref(fig:opr)). 
The survey included questions on common dimensions of open peer review, like
whether peer review reports, editorial decision letters or previous versions of
the manuscript are published, or whether there is public commenting during peer
review, and similar questions. All surveyed aspects of 
open peer review lack any kind of information in more than 50% of journals 
surveyed. 
Furthermore, three quarters of journals do not provide information on all except
one aspect. When there is information, in most cases it is 
dismissive of open peer review. No journal in our sample allows public 
commenting during formal peer review. Other forms of openness are similarly 
scarce. With the sole exception that some journals may reveal reviewer 
identities to the authors, all other aspects are not specified or not
available in more than 95% of journals.

```{r opr, fig.asp=.7, fig.cap="Aspects of open peer review"}
p_cols <- c("Not specified" =  "#E7298A", "Yes" = "#1B9E77",
            "Mandatory" = "#D95F02", "Conditional" =  "#7570B3",
            "Optional" = "#66A61E", "No" = "#E6AB02"
)

pdata %>% 
  left_join(labels) %>% 
  ggplot(., aes(fct_reorder(label, order), prop, fill = fct_rev(val_clean))) +
  geom_chicklet(position = "fill", width = .6) +
  coord_flip() +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_manual(values = p_cols) +
  theme(legend.position = "top") +
  guides(fill = guide_legend(reverse = T)) +
  labs(x = NULL, y = NULL, fill = NULL) 
```

Splitting the aspect of revealed reviewer identities by discipline shows a few
key distinctions (see fig. \@ref(fig:opr-authors)). Whereas revealing reviewer
identities to the authors is absent from the social sciences, humanities and
business, it is not unusual in the sciences, at least on an optional basis 
(for example in case the referee wants to sign their review). 


```{r opr-authors, fig.cap="Open reviewer identities towards authors", fig.asp=.6}
rev_identitiy <- refined_with_areas %>% 
  mutate(opr_indenties_author_clean = case_when(
    str_detect(opr_indenties_author, "Conditional") ~ "Conditional",
    str_detect(str_to_lower(opr_indenties_author), "not spec") ~ "Not specified",
    str_detect(opr_indenties_author, "Optional") ~ "Optional",
    TRUE ~ opr_indenties_author
  )) %>% 
  mutate(opr_indenties_author_clean = factor(opr_indenties_author_clean, 
                      levels = c("Yes", "Mandatory", "Conditional", "Optional",
                                 "No", "Not specified"))) %>% 
  make_proportion(opr_indenties_author_clean, area, order_string = "Not")


p_cols <- c("Not specified" =  "#E7298A", "Yes" = "#1B9E77",
            "Mandatory" = "#D95F02", "Conditional" =  "#7570B3",
            "Optional" = "#66A61E", "No" = "#E6AB02"
)

ggplot(rev_identitiy, aes(fct_rev(fct_reorder(area, order)), prop, 
                          fill = fct_rev(opr_indenties_author_clean))) +
  geom_chicklet(position = "fill", width = .6) +
  coord_flip() +
  scale_fill_manual(values = p_cols) +
  scale_y_continuous(labels = scales::percent) +
  theme(legend.position = "top") +
  guides(fill = guide_legend(reverse = T)) +
  labs(fill = NULL, x = NULL, y = NULL, 
       caption = "Are reviewer identities revealed to the author (even if not published)?")
```

TODO: explain differences between Yes, Mandatory, Conditional, Optional. Tony to
check and explain.


# Co-Review Policy
```{r}
coreview_policies <- refined %>% 
  select(coreview_policy) %>% 
  filter(!is.na(coreview_policy),
         !(coreview_policy %in% c("Not specified", "Not found")))
```
Information on co-review policies is sparse. 
Only `r nrow(coreview_policies)` out of `r nrow(refined)` journals do have an
explicit co-review policy.

Splitting the results by discipline
reveals noticeable differences (see fig. \@ref(fig:co-rev)).
While in the life and earth sciences more
than one third of journals permit contributions from co-reviewers, in the 
humanities, chemical & materials sciences, and in business, economics & 
management about 90% of journals have no policy on co-reviewing.

```{r co-rev, fig.cap="Prevalence of co-review", fig.asp=.6}
co_rev <- refined_with_areas %>% 
  mutate(coreview_email = case_when(
    coreview_email == "unsure" ~ "Unsure",
    # lump the two not specified to unsure, since this is similar in this 
    # instance
    coreview_email == "Not specified" ~ "Unsure",
    TRUE ~ coreview_email
  ),
  coreview_email = factor(coreview_email, 
                      levels = c("Yes", "No", "Unsure"))) %>% 
  make_proportion(coreview_email, area, order_string = "Yes")


p_cols <- c("Unsure" = "#A6761D", "Yes" = "#1B9E77",
            "No" = "#E6AB02"
)

ggplot(co_rev, aes(fct_reorder(area, order), prop, 
                          fill = fct_rev(coreview_email))) +
  geom_chicklet(position = "fill", width = .6) +
  coord_flip() +
  scale_fill_manual(values = p_cols) +
  scale_y_continuous(labels = scales::percent) +
  theme(legend.position = "top") +
  guides(fill = guide_legend(reverse = T)) +
  labs(fill = NULL, x = NULL, y = NULL, 
       caption = "Can co-reviewers contribute?")
```


To obtain a more nuanced view of the policies' contents, we also analysed their
full text via text mining. Due to policies being similar across journals of 
certain publishers, there are `r nrow(distinct(coreview_policies))`
distinct policies in our dataset (compared to `r nrow(coreview_policies)` 
policies in total). Since the policies are rather short, we are 
somewhat limited in regard to what insight we can gain from automated 
procedures.


```{r}
custom_stopwords <- tidytext::stop_words %>% 
  filter(word != "not")

stopped_words <- coreview_policies %>% 
  distinct() %>%
  mutate(policy_id = row_number()) %>% 
  unnest_tokens(word, coreview_policy) %>% 
  anti_join(custom_stopwords)
```

To extract meaningful information we first removed common words of the English
language (via the list of stop-words from the tidytext package
[@silge_tidytext_2016], except for the word "not", which is relevant since some
policies state, that it is *not* appropriate to share information with students
or colleagues). The resulting list contains `r nrow(stopped_words)` words in 
total. 

For a simple overview, the words were stemmed to reduce similar but not 
identical versions of certain words (like editor/editors).
Table \@ref(tab:coreview-table) displays the most frequent parts of the
distinct policies, sorted by the proportion of policies that contain a given
term.
```{r coreview-table}
stemmed_words <- stopped_words %>% 
  mutate(word = SnowballC::wordStem(word)) 
  

stemmed_words %>% 
  mutate(word_appears = T) %>% 
  complete(policy_id, word, fill = list(word_appears = FALSE)) %>% 
  group_by(word) %>% 
  summarise(n = sum(word_appears),
            n_policies = max(policy_id),
            prop_of_texts = mean(word_appears)) %>% 
  arrange(desc(prop_of_texts)) %>% 
  mutate(prop_of_texts = scales::percent(prop_of_texts, accuracy = 1)) %>% 
  select(Term = word, `Term frequency` = n, 
         `Proportion of policies that contain term` = prop_of_texts) %>% 
  head(20) %>% 
  knitr::kable(caption = "Propensity of terms in co-review policies")
```


The most prominent themes that emerge are:

- Individuals with varying stakes regarding peer review: editor, colleague, 
author, student, peer.
- Confidentiality as a central principle.
- Important elements of scholarly publishing: manuscript, journal, review, 
process.
- Verbal forms pertaining to relationships between the individuals: inform,
involve, consult, discuss, disclose, ensure, obtain.

These directions become more intelligible when we look at bigrams (see fig.
\@ref(fig:bigrams)). With this procedure the text is
split into pairs of words (for example the sentence "All humans are equal" 
becomes "All humans", "humans are", "are equal"). The most prominent bigrams 
where "peer -> review" and "review -> process". To take a look at the strength 
of other associations, the term "review" was removed from the figure.


```{r bigrams, fig.width=9, fig.height=7, fig.cap="Bigrams of co-review policies"}
coreview_policies %>% 
  make_bigram_analysis(coreview_policy, remove = "review", cutoff = 2)
```

From both displays it is obvious, that journals stress the importance of 
"maintaining confidentiality", by "not shar[ing]" or disclosing information,
neither to "junior researchers", "laboratory colleagues" nor "graduate 
students". Even if the policies do not explicitly forbid or allow the 
involvement of other researchers, in many cases they mandate the reviewer to 
first obtain permission from the editor in case they want to involve someone
else in their review. The editor's prominent role can also be observed by the 
terms' frequent appearance in the policies. Three quarters of all policies
mention the term "editor". 



# Preprints
```{r}
clean_preprint_version <- function(df) {
  df %>% 
    mutate(preprint_version_clean = case_when(
      str_detect(preprint_version, "Unsure") ~ 
        "Unsure (preprints are allowed, but it's not clear which version)",
      str_detect(preprint_version, "Any .*? or") ~ "Other",
      str_detect(preprint_version, "Other|Unclear") ~ "Other",
      str_detect(preprint_version, "None") ~ "None",
      str_detect(preprint_version, "First sub") ~ 
        "First submission only (before peer review)",
      str_detect(preprint_version, "After peer re") ~ "After peer review",
      str_detect(preprint_version, "Any|any") ~ "Any",
      str_detect(preprint_version, "No") ~ "No preprint policy",
    )) 
}

preprint_version_univariate <- refined %>% 
  clean_preprint_version() %>% 
  make_proportion(preprint_version_clean, order_string = "Any|After|First|Uns")
```


Preprints are more common within our sample than open peer review or co-review
policies. Almost
`r preprint_version_univariate %>% pull(order) %>% make_percent()`
of all journals allow preprints at least in some way. Most of them however only
allow preprints before peer review 
(`r preprint_version_univariate[[3, 3]] %>% make_percent(2)`)
while 
`r preprint_version_univariate[[4, 3]] %>% make_percent(2)`
do not have a preprint policy.


```{r preprint-version, fig.cap="Allowed preprint versions", fig.asp=.7}
refined_with_areas <- refined_with_areas %>% 
  clean_preprint_version() %>% 
  mutate(preprint_version_clean = fct_relevel(
    preprint_version_clean, "Unsure (preprints are allowed, but it's not clear which version)",
    "Any", "First submission only (before peer review)") %>% 
    fct_relevel("None", "No preprint policy", "Other", after = 4))
    
    
    
preprint_version <- refined_with_areas %>% 
  make_proportion(preprint_version_clean, area,
                  order_string = "Any|After|First|Uns")


p_cols <- c("No preprint policy" =  "#E7298A", "Other" = "#666666",
            "Any" = "#D95F02", "After peer review" = "#66A61E", 
            "First submission only (before peer review)" = "#7570B3",
            "Unsure (preprints are allowed, but it's not clear which version)" = "#1B9E77",
            "None" = "#E6AB02"
)

p <- ggplot(preprint_version, aes(fct_reorder(area, order), prop, 
                          fill = fct_rev(preprint_version_clean))) +
  geom_chicklet(position = "fill", width = .6) +
  geom_step(data = slice(preprint_version, 1),
            aes(x = area, y = order, group = 1), direction = "vh",
            position = position_nudge(x = .5)) +
  geom_step(data = filter(preprint_version, str_detect(area, "Human|Busine")), 
            aes(x = area, y = order, group = 1), direction = "hv",
            position = position_nudge(x = -.5)) +
  annotate("text", x = "Life Sciences & Earth Sciences", y = 1, 
           label = "Proportion of journals\nthat allow posting of preprints",
           vjust = -.8, hjust = 1,
           family = "Hind", size = 3) +
  coord_flip() +
  scale_fill_manual(values = p_cols) +
  scale_y_continuous(labels = scales::percent) +
  theme(legend.position = "bottom") +
  guides(fill = guide_legend(reverse = T, nrow = 4)) +
  labs(fill = NULL, x = NULL, y = NULL, 
       caption = "What version of a preprint can be posted?")
p

gt <- ggplot_gtable(ggplot_build(p))
gt$layout$clip[gt$layout$name == "panel"] <- "off"
grid::grid.draw(gt)

```

Similar to our earlier results, preprint policies show a wide disciplinary 
range (see fig. \@ref(fig:preprint-version)). While in the life sciences & earth 
sciences 
`r preprint_version %>% filter(str_detect(area, "Life Sciences")) %>% pull(order) %>% make_percent()`
of all journals allow preprints in some way, in the Humanities only 
`r preprint_version %>% filter(str_detect(area, "Humanities")) %>% pull(order) %>% make_percent(2)`
do.
The sciences in general tend towards allowing preprints only on first submission
while the social sciences predominantly have no clear policy on which version of
a preprint is allowed. 
The humanities and also journal from business, economics and management 
generally have either no preprint policy at all or are more diverse in regard to
preprint version, often allowing preprints after peer review, which is a lot 
less common in the sciences.

```{r}
clean_preprint_citation <- function(df) {
  df %>% 
   mutate(preprint_citation_clean = case_when(
      str_detect(preprint_citation, "Not spec") ~ "Not specified",
      str_detect(preprint_citation, "Unsure") ~ "Unsure",
      str_detect(preprint_citation, "No") ~ "No",
      str_detect(preprint_citation, "only in the text") ~ "Yes, but only in the text",
      str_detect(preprint_citation, "reference list") ~ "Yes, in the reference list",
      is.na(preprint_citation) ~ NA_character_,
      TRUE ~ "Other"
    )) 
}

preprint_citation_univariate <- refined %>% 
  clean_preprint_citation() %>% 
  make_proportion(preprint_citation_clean, order_string = "Yes")
```


A complementary aspect of using preprints is, whether they can be cited. The
majority of journals
(`r preprint_citation_univariate[[2, 3]] %>% make_percent(2)`)
does not specify, whether this is possible. Unclear policies on how to cite
preprints are also quite 
common (`r preprint_citation_univariate[[4, 3]] %>% make_percent(2)`). In case
citations of preprints are allowed, this is commonly possible in the reference,
with some journals restricting citations of preprints to the text.


```{r preprint-citation, fig.cap="Citation of preprints", fig.asp=.65}
refined_with_areas <- refined_with_areas %>% 
  clean_preprint_citation()

preprint_citation <- refined_with_areas %>% 
    mutate(preprint_citation_clean = 
             fct_relevel(preprint_citation_clean,
                         "Yes, in the reference list", "Yes, but only in the text",
                         "No", "Unsure", "Other", "Not specified")) %>% 
  make_proportion(preprint_citation_clean, area,
                  order_string = "Yes")


p_cols <- c("Not specified" =  "#E7298A", "Other" = "#666666", "Unsure" = "#A6761D",
            "Yes, in the reference list" = "#1B9E77", 
            "Yes, but only in the text" = "#D95F02", 
            "No" = "#E6AB02"
)

p <- ggplot(preprint_citation, aes(fct_reorder(area, order), prop, 
                          fill = fct_rev(preprint_citation_clean))) +
  geom_chicklet(position = "fill", width = .6) +
  geom_step(data = slice(preprint_citation, 1),
            aes(x = area, y = order, group = 1), direction = "vh",
            position = position_nudge(x = .5)) +
  geom_step(data = filter(preprint_citation, str_detect(area, "Business|Social")), 
            aes(x = area, y = order, group = 1), direction = "hv",
            position = position_nudge(x = -.5)) +
  annotate("text", x = "Life Sciences & Earth Sciences", y = .5, 
           label = "Proportion of journals\nthat allow citation of preprints",
           vjust = -.8,
           family = "Hind", size = 3) +
  coord_flip() +
  scale_fill_manual(values = p_cols) +
  scale_y_continuous(labels = scales::percent) +
  theme(legend.position = "bottom") +
  guides(fill = guide_legend(reverse = T)) +
  labs(fill = NULL, x = NULL, y = NULL,
       caption = "Can preprints be cited?")

gt <- ggplot_gtable(ggplot_build(p))
gt$layout$clip[gt$layout$name == "panel"] <- "off"
grid::grid.draw(gt)

```

Disciplinary differences are again very apparent (see fig.
\@ref(fig:preprint-citation)). Citing preprints is more common in the sciences,
with 
`r preprint_citation %>% filter(str_detect(area, "Life")) %>% pull(order) %>% make_percent()`
of all journals in the life and earth sciences allowing citations to preprints
either in the text or in the reference list. In contrast, the social sciences 
and humanities either have unclear or no policies regarding whether preprints 
can be cited or not. 


# Discussion


## Influential role of editor
The editor's role in all this is higly influential, with a lot of leeway.
Flexibility is good, uncertainty is bad.

Recall figure 
\@ref(fig:opr-authors), where we investigated whether reviewer identities are
revealed to authors, even if they are not made public. The high 
proportion of journals within SSH that are categorised as "not specified" might
be surprising, given that most of them conduct double blind peer review. One 
could thus infer, that reviewer identities are not revealed to the author. This
inference however is the root problem: there are no clear policies. Reviewers
might sign their review or not, what the authors receive is at the editor's 
discretion. 

## Unclear policies
this is bad, because ...


# Bibliography

